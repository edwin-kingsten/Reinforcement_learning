{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Monte Carlo No Explore start (eps greedy).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxW-Bi1-f3L6"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfP4ZQQFgD-O"
      },
      "source": [
        "## Defining Grid World\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Ry-QJvgP2H"
      },
      "source": [
        "class Grid():\n",
        "    def __init__(self , rows , cols , start):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.start = start\n",
        "        self.i = start[0]\n",
        "        self.j = start[1]\n",
        "\n",
        "    def set(self, actions , rewards):\n",
        "        self.actions = actions\n",
        "        self.rewards = rewards\n",
        "\n",
        "    def reset(self):\n",
        "        self.i  , self.j = self.start\n",
        "\n",
        "    def set_state(self, s):\n",
        "        self.i = s[0]\n",
        "        self.j = s[1]\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        return s in self.rewards\n",
        "\n",
        "    def current_state(self):\n",
        "        return (self.i , self.j)\n",
        "\n",
        "    def all_states(self):\n",
        "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "    def move(self , a):\n",
        "        if a in self.actions[(self.i , self.j)]:\n",
        "            if a == \"U\":\n",
        "                self.i -= 1\n",
        "\n",
        "            elif a == \"D\":\n",
        "                self.i += 1\n",
        "\n",
        "            elif a == \"R\":\n",
        "                self.j += 1\n",
        "\n",
        "            elif a == \"L\":\n",
        "                self.j -= 1\n",
        "        \n",
        "        return self.rewards.get((self.i , self.j) , 0)\n",
        "\n",
        "    def game_over(self):\n",
        "        return (self.i , self.j) in self.rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jbv9QzugSpD"
      },
      "source": [
        "def initialize_grid():\n",
        "    rows , cols = 3 , 4\n",
        "    start = (2 , 0)\n",
        "    g = Grid(rows , cols , start)\n",
        "    \n",
        "    # reward for termination state\n",
        "    rewards = {(0, 3): 1 , (1 , 3) : -1}\n",
        "    \n",
        "    # possible actions can be performed for each state\n",
        "    actions = {\n",
        "              (0, 0):[\"D\" , \"R\"],\n",
        "              (0, 1):[\"R\"],\n",
        "              (0, 2):[\"R\"],\n",
        "              (1, 0):[\"U\" ,\"D\"],\n",
        "              (1, 2):[\"U\" ,\"D\" ,\"R\"],\n",
        "              (2, 0):[\"U\" , \"R\"],\n",
        "              (2, 1):[\"R\" , \"L\"],\n",
        "              (2, 2):[\"R\" , \"L\" ,\"U\"],\n",
        "              (2, 3):[\"L\" , \"U\"]\n",
        "    }\n",
        "\n",
        "    g.set(actions , rewards)\n",
        "\n",
        "    return g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of87R6kMgVCM"
      },
      "source": [
        "## Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iwwR3S7gZFn"
      },
      "source": [
        "def print_values(V, g):\n",
        "    print(\"Printing Values\")\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            v = V.get((i,j), 0)\n",
        "            if v >= 0:\n",
        "                print(\" %.2f|\" % v, end=\"\")\n",
        "            else:\n",
        "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "    print(\"Printing Policy\")\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            a = P.get((i,j), ' ')\n",
        "            print(\"  %s  |\" % a, end=\"\") \n",
        "        print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIofnPPogz6A"
      },
      "source": [
        "## Define Epsilon Greedy random action\n",
        "def random_action(a , eps = 0.1):\n",
        "    if np.random.random() < (1 - eps) :\n",
        "        return a\n",
        "    else:\n",
        "        return np.random.choice(ACTION_SPACE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlCNaSengZCO"
      },
      "source": [
        "## Defining Play Game function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R_WOxhxgY_j"
      },
      "source": [
        "def play_game(grid , Policy):\n",
        "    # choose a random action for random state with probability eps\n",
        "    # this is called the \"exploring starts\" method\n",
        "    grid.reset()\n",
        "    s = grid.current_state()\n",
        "    a = random_action(Policy[s])\n",
        "    \n",
        "    states = [s]\n",
        "    actions = [a]\n",
        "    rewards = [0]\n",
        "\n",
        "    for _ in range(20): #sometimes states can form a loop , so we have to limit the number of steps\n",
        "        old_s  = s\n",
        "        r = grid.move(a)\n",
        "        s = grid.current_state()\n",
        "\n",
        "        states.append(s)\n",
        "        rewards.append(r)\n",
        "        \n",
        "        if grid.game_over():\n",
        "            break\n",
        "        \n",
        "        # # # if the next state is wall , then move is going to return the same state\n",
        "        # elif old_s == s:\n",
        "        #     # rewards[-1] = -0.4\n",
        "        #     # # a = random_action(Policy[s])\n",
        "        #     # # actions.append(a)\n",
        "        #     break\n",
        "\n",
        "        else:\n",
        "            a = random_action(Policy[s])\n",
        "            actions.append(a)\n",
        "        # print(s , a)\n",
        "    \n",
        "    # we want to return:\n",
        "    # states  = [s(0), s(1), ..., s(T-1), s(T)]\n",
        "    # actions = [a(0), a(1), ..., a(T-1),     ]\n",
        "    # rewards = [   0, R(1), ..., R(T-1), R(T)]\n",
        "    # return (states , actions , rewards)\n",
        "    return states , actions , rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dwR0yI5gY6F"
      },
      "source": [
        "# function for finding maximum value and its corresponding key in dictionary\n",
        "def max_dict(d):\n",
        "    # find max val\n",
        "    max_val = max(d.values())\n",
        "\n",
        "    # find keys corresponding to max val\n",
        "    for key , val in d.items():\n",
        "        if val == max_val:\n",
        "            max_key = key\n",
        "            break\n",
        "\n",
        "    return max_key , max_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dSnqmZicQz"
      },
      "source": [
        "## Initialize Grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFv8PXFJieux"
      },
      "source": [
        "grid = initialize_grid()\n",
        "ACTION_SPACE = (\"U\" , \"D\" , \"R\" , \"L\")\n",
        "gamma = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzifpaymgY3N"
      },
      "source": [
        "## Initialize Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmW8EPlHgYzp"
      },
      "source": [
        "Policy = {}\n",
        "for i in grid.actions.keys():\n",
        "    Policy[i] = np.random.choice(ACTION_SPACE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUwBjOBugYxO"
      },
      "source": [
        "## Initialize Q values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yORkY_f7gYuy"
      },
      "source": [
        "Q = {}\n",
        "sample_counts = {}\n",
        "\n",
        "for s in grid.actions.keys():\n",
        "        Q[s] = {}\n",
        "        sample_counts[s] = {}\n",
        "        for a in ACTION_SPACE:\n",
        "            Q[s][a] = 0.0\n",
        "            sample_counts[s][a] = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8we5DDcjbtd"
      },
      "source": [
        "## Monte Carlo Policy Evaluation and Policy Improvement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cddEhnyKj7Rb",
        "outputId": "93eb8eb7-d1a5-4c42-b259-e2e288a62247"
      },
      "source": [
        "delta = []\n",
        "gamma = 0.9\n",
        "biggest_change = 0\n",
        "\n",
        "for it in range(10000):\n",
        "    if it%1000 == 0:\n",
        "        print(\"iteration \" , it)\n",
        "        print_policy(Policy , grid)\n",
        "\n",
        "    states , actions , rewards = play_game(grid , Policy)\n",
        "    state_actions = list(zip(states , actions))\n",
        "    \n",
        "    G = 0\n",
        "    T = len(states)\n",
        "    for t in range(T-2 , -1 , -1): # going in reverse direction\n",
        "        a = actions[t]\n",
        "        s = states[t]\n",
        "        \n",
        "        G = rewards[t+1] + gamma*G\n",
        "\n",
        "        if (s , a) not in state_actions[:t]:\n",
        "            old_q = Q[s][a]\n",
        "            sample_counts[s][a] += 1\n",
        "            N = sample_counts[s][a]\n",
        "            Q[s][a] =  ((N - 1)*old_q + G)/N # running mean\n",
        "\n",
        "            Policy[s] = max_dict(Q[s])[0]\n",
        "            biggest_change = max(biggest_change , np.abs(old_q - Q[s][a]))      \n",
        "\n",
        "    delta.append(biggest_change)\n",
        "\n",
        "plt.plot(delta)\n",
        "plt.show()\n",
        "\n",
        "print(\"final policy:\")\n",
        "print_policy(Policy, grid)\n",
        "\n",
        "# find V\n",
        "V = {}\n",
        "for s, Qs in Q.items():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "print(\"final values:\")\n",
        "print_values(V, grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration  0\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  1000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  2000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  3000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  4000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  5000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  6000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  7000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  8000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration  9000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final policy:\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "final values:\n",
            "Printing Values\n",
            "---------------------------\n",
            " 0.80| 0.89| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.71| 0.00| 0.89| 0.00|\n",
            "---------------------------\n",
            " 0.63| 0.66| 0.75| 0.60|\n"
          ]
        }
      ]
    }
  ]
}