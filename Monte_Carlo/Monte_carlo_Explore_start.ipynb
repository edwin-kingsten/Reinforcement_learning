{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Monte_carlo Explore start.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD3P86neCA0g"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myhVc7KdGTYO"
      },
      "source": [
        "## Defining Grid World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18X9Ui09CKNf"
      },
      "source": [
        "class Grid():\n",
        "    def __init__(self , rows , cols , start):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.i = start[0]\n",
        "        self.j = start[1]\n",
        "\n",
        "    def set(self, actions , rewards):\n",
        "        self.actions = actions\n",
        "        self.rewards = rewards\n",
        "\n",
        "    def set_state(self, s):\n",
        "        self.i = s[0]\n",
        "        self.j = s[1]\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        return s in self.rewards\n",
        "\n",
        "    def current_state(self):\n",
        "        return (self.i , self.j)\n",
        "\n",
        "    def all_states(self):\n",
        "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "    def move(self , a):\n",
        "        if a in self.actions[(self.i , self.j)]:\n",
        "            if a == \"U\":\n",
        "                self.i -= 1\n",
        "\n",
        "            elif a == \"D\":\n",
        "                self.i += 1\n",
        "\n",
        "            elif a == \"R\":\n",
        "                self.j += 1\n",
        "\n",
        "            elif a == \"L\":\n",
        "                self.j -= 1\n",
        "        \n",
        "        return self.rewards.get((self.i , self.j) , 0)\n",
        "\n",
        "    def game_over(self):\n",
        "        return (self.i , self.j) in self.rewards"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioYmSbUAHD20"
      },
      "source": [
        "def initialize_grid():\n",
        "    rows , cols = 3 , 4\n",
        "    start = (2 , 0)\n",
        "    g = Grid(rows , cols , start)\n",
        "    \n",
        "    # reward for termination state\n",
        "    rewards = {(0, 3): 1 , (1 , 3) : -1}\n",
        "    \n",
        "    # possible actions can be performed for each state\n",
        "    actions = {\n",
        "              (0, 0):[\"D\" , \"R\"],\n",
        "              (0, 1):[\"R\"],\n",
        "              (0, 2):[\"R\"],\n",
        "              (1, 0):[\"U\" ,\"D\"],\n",
        "              (1, 2):[\"U\" ,\"D\" ,\"R\"],\n",
        "              (2, 0):[\"U\" , \"R\"],\n",
        "              (2, 1):[\"R\" , \"L\"],\n",
        "              (2, 2):[\"R\" , \"L\" ,\"U\"],\n",
        "              (2, 3):[\"L\" , \"U\"]\n",
        "    }\n",
        "\n",
        "    g.set(actions , rewards)\n",
        "\n",
        "    return g"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2K8TultHP7C"
      },
      "source": [
        "## Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRghMJjJHbZe"
      },
      "source": [
        "def print_values(V, g):\n",
        "    print(\"Printing Values\")\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            v = V.get((i,j), 0)\n",
        "            if v >= 0:\n",
        "                print(\" %.2f|\" % v, end=\"\")\n",
        "            else:\n",
        "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "    print(\"Printing Policy\")\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            a = P.get((i,j), ' ')\n",
        "            print(\"  %s  |\" % a, end=\"\") \n",
        "        print(\"\")"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbw0hzoBHe1S"
      },
      "source": [
        "## Defining Play Game function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIcOivS2HnX7"
      },
      "source": [
        "def play_game(grid , Policy):\n",
        "    # reset game to start at a random position\n",
        "    # we need to do this if we have a deterministic policy\n",
        "    # we would never end up at certain states, but we still want to measure their value\n",
        "    # this is called the \"exploring starts\" method\n",
        "\n",
        "    action_states = list(grid.actions.keys())\n",
        "    start_state_index = np.random.choice(len(action_states))\n",
        "    # print(action_states)\n",
        "    grid.set_state(action_states[start_state_index])\n",
        "\n",
        "    s = grid.current_state()\n",
        "    a = np.random.choice(ACTION_SPACE)\n",
        "    \n",
        "    states = [s]\n",
        "    actions = [a]\n",
        "    rewards = [0]\n",
        "\n",
        "    for _ in range(len(grid.all_states())): #sometimes states can form a loop , so we have to limit the number of steps\n",
        "        old_s  = s\n",
        "        r = grid.move(a)\n",
        "        s = grid.current_state()\n",
        "\n",
        "        states.append(s)\n",
        "        rewards.append(r)\n",
        "        \n",
        "        if grid.game_over():\n",
        "            break\n",
        "        \n",
        "        # if the next state is wall , then move is going to return the same state\n",
        "        elif old_s == s:\n",
        "            rewards[-1] = -100\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            a = Policy[s]\n",
        "            actions.append(a)\n",
        "        # print(s , a)\n",
        "    \n",
        "    # we want to return:\n",
        "    # states  = [s(0), s(1), ..., s(T-1), s(T)]\n",
        "    # actions = [a(0), a(1), ..., a(T-1),     ]\n",
        "    # rewards = [   0, R(1), ..., R(T-1), R(T)]\n",
        "    # return (states , actions , rewards)\n",
        "    return states , actions , rewards"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kn895u1LGWe"
      },
      "source": [
        "# function for finding maximum value and its corresponding key in dictionary\n",
        "def max_dict(d):\n",
        "    # find max val\n",
        "    max_val = max(d.values())\n",
        "\n",
        "    # find keys corresponding to max val\n",
        "    for key , val in d.items():\n",
        "        if val == max_val:\n",
        "            max_key = key\n",
        "            break\n",
        "\n",
        "    return max_key , max_val"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F-ukMkZclwY"
      },
      "source": [
        "## Initialize grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T-K2phyHnTe"
      },
      "source": [
        "grid = initialize_grid()\n",
        "ACTION_SPACE = (\"U\" , \"D\" , \"R\" , \"L\")\n",
        "gamma = 0.9"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cUKK9Jtcom0"
      },
      "source": [
        "## Initialize Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvo9clmbHnMI"
      },
      "source": [
        "Policy = {}\n",
        "for s in grid.actions.keys():\n",
        "    Policy[s] = np.random.choice(ACTION_SPACE)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjOWUoy9dvE9"
      },
      "source": [
        "## Initialize Q values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DsFtOwzHnJQ"
      },
      "source": [
        "# initialize Q(s,a) and returns\n",
        "Q = {}\n",
        "sample_counts = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "    if s not in grid.rewards: # not a terminal state\n",
        "        Q[s] = {}\n",
        "        sample_counts[s] = {}\n",
        "        for a in ACTION_SPACE:\n",
        "            Q[s][a] = 0\n",
        "            sample_counts[s][a] = 0\n",
        "    else:\n",
        "        # terminal state or state we can't otherwise get to\n",
        "        pass\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qk0xH4Yd1yp"
      },
      "source": [
        "## Monte Carlo Policy Improvement and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BfwHgRL7HnDf",
        "outputId": "a15aafeb-0c16-474b-a10a-1698e13b6f62"
      },
      "source": [
        "# repeat until convergence\n",
        "deltas = []\n",
        "for it in range(10000):\n",
        "    if it % 1000 == 0:\n",
        "        print(\"iteration\" , it)\n",
        "        print_policy(Policy , grid)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states , actions , rewards = play_game(grid, Policy)\n",
        "    # states ,actions , rewards = state_action_rewards[... , 0] , state_action_rewards[... , 1] , state_action_rewards[... , 2]\n",
        "\n",
        "    # create a list of only state-action pairs for lookup\n",
        "    states_actions = list(zip(states, actions))\n",
        "\n",
        "    T = len(states)\n",
        "    G = 0\n",
        "    for t in range(T - 2, -1, -1):\n",
        "        # retrieve current s, a, r tuple\n",
        "        s = states[t]\n",
        "        a = actions[t]\n",
        "\n",
        "        # update G\n",
        "        G = rewards[t+1] + gamma * G\n",
        "\n",
        "        # check if we have already seen (s, a) (\"first-visit\")\n",
        "        if (s, a) not in states_actions[:t]:\n",
        "            old_q = Q[s][a]\n",
        "            sample_counts[s][a] += 1\n",
        "            N = sample_counts[s][a]\n",
        "            Q[s][a] = ((N - 1)*old_q + (G) ) / N # running mean\n",
        "\n",
        "            # update policy\n",
        "            Policy[s] = max_dict(Q[s])[0]\n",
        "\n",
        "            # update delta\n",
        "            biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "    deltas.append(biggest_change)\n",
        "    # if biggest_change < 1e-3:\n",
        "    #     break\n",
        "\n",
        "plt.plot(deltas)\n",
        "plt.show()\n",
        "\n",
        "print(\"final policy:\")\n",
        "print_policy(Policy, grid)\n",
        "\n",
        "# find V\n",
        "V = {}\n",
        "for s, Qs in Q.items():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "print(\"final values:\")\n",
        "print_values(V, grid)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  D  |  U  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  D  |  U  |  R  |  D  |\n",
            "iteration 1000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  R  |  R  |  R  |  L  |\n",
            "iteration 2000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  R  |  R  |  R  |  L  |\n",
            "iteration 3000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  R  |  R  |  R  |  L  |\n",
            "iteration 4000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  L  |\n",
            "iteration 5000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  L  |\n",
            "iteration 6000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  L  |\n",
            "iteration 7000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration 8000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "iteration 9000\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQPUlEQVR4nO3dfYxc1XnH8e+DN14DdvyCN9ZiQ9coFgmqlEJXqREVinBeCKSBP1BEmhaX0rpK05YklRJTIkWVWgmqKAHUKIkDJG5FCZSggjAkAkOUplXdrEkawC+xMSHYsvG6CS+hLWD76R9zTIbNLvbOnfV4j78faTT3nvsyz5mz/nn2zJ2dyEwkSXU5odcFSJK6z3CXpAoZ7pJUIcNdkipkuEtShfp6XQDAwoULc2hoqNdlSNK0snHjxn2ZOTDetmMi3IeGhhgZGel1GZI0rUTE0xNtc1pGkipkuEtShQx3SaqQ4S5JFTLcJalChw33iLg1IvZGxONtbQsi4sGI2Fbu55f2iIibImJ7RPwoIs6ZyuIlSeM7klfuXwcuHNO2GlifmcuA9WUd4P3AsnJbBXypO2VKkibjsOGemd8Ffjam+RJgbVleC1za1v4P2fIfwLyIGOxWsWP9+5P7GFq9jqHV6/jbdZum6mEkadrpdM59UWbuLst7gEVleTHwTNt+O0vbr4iIVRExEhEjo6OjHRXxu1/d8NryV//1qY7OIUk1avyGara+7WPS3/iRmWsyczgzhwcGxv30rCSpQ52G+7OHplvK/d7Svgs4rW2/JaVNknQUdRru9wIry/JK4J629ivKVTPLgefbpm8kSUfJYf9wWETcDrwLWBgRO4HPAtcBd0bEVcDTwIfK7vcDFwHbgf8BrpyCmiVJh3HYcM/MD0+wacU4+ybwsaZFSZKa8ROqklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVqFG4R8QnIuKJiHg8Im6PiFkRsTQiNkTE9oi4IyJmdqtYSdKR6TjcI2Ix8BfAcGb+OjADuBy4HvhCZr4V+DlwVTcKlSQduabTMn3AiRHRB5wE7AYuAO4q29cClzZ8DEnSJHUc7pm5C/gc8FNaof48sBF4LjP3l912AovHOz4iVkXESESMjI6OdlqGJGkcTaZl5gOXAEuBU4GTgQuP9PjMXJOZw5k5PDAw0GkZkqRxNJmWeTfwVGaOZuarwN3AecC8Mk0DsATY1bBGSdIkNQn3nwLLI+KkiAhgBbAJeAS4rOyzErinWYmSpMlqMue+gdYbp48Cj5VzrQE+DXwyIrYDpwC3dKFOSdIk9B1+l4ll5meBz45p3gG8s8l5JUnN+AlVSaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVKFG4R4R8yLirojYEhGbI+LciFgQEQ9GxLZyP79bxUqSjkzTV+43At/KzLcB7wA2A6uB9Zm5DFhf1iVJR1HH4R4Rc4HzgVsAMvOVzHwOuARYW3ZbC1zatEhJ0uQ0eeW+FBgFvhYRP4iImyPiZGBRZu4u++wBFo13cESsioiRiBgZHR1tUIYkaawm4d4HnAN8KTPPBl5izBRMZiaQ4x2cmWsyczgzhwcGBhqUIUkaq0m47wR2ZuaGsn4XrbB/NiIGAcr93mYlSpImq+Nwz8w9wDMRcWZpWgFsAu4FVpa2lcA9jSqUJE1aX8Pj/xy4LSJmAjuAK2n9h3FnRFwFPA18qOFjSJImqVG4Z+YPgeFxNq1ocl5JUjN+QlWSKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklShaR3uc2b19boESTomTetwlySNz3CXpApN63CPXhcgSceoaR3ukqTxGe6SVKFpHe7Z6wIk6RjVONwjYkZE/CAi7ivrSyNiQ0Rsj4g7ImJm8zIlSZPRjVfuVwOb29avB76QmW8Ffg5c1YXHkCRNQqNwj4glwMXAzWU9gAuAu8oua4FLmzzGG3nx//ZP1aklaVpr+sr9BuBTwMGyfgrwXGYeSt2dwOLxDoyIVRExEhEjo6OjDcuQJLXrONwj4gPA3szc2MnxmbkmM4czc3hgYKDTMiRJ42jyx1nOAz4YERcBs4A3AzcC8yKir7x6XwLsal6mJGkyOn7lnpnXZOaSzBwCLgcezsyPAI8Al5XdVgL3NK5SkjQpU3Gd+6eBT0bEdlpz8LdMwWNIkt5AV/5mbmZ+B/hOWd4BvLMb55UkdWZaf0JVkjQ+w12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUIdh3tEnBYRj0TEpoh4IiKuLu0LIuLBiNhW7ud3r1xJ0pFo8sp9P/CXmXkWsBz4WEScBawG1mfmMmB9WZckHUUdh3tm7s7MR8vyi8BmYDFwCbC27LYWuLRpkZKkyenKnHtEDAFnAxuARZm5u2zaAyya4JhVETESESOjo6PdKEOSVDQO94iYDXwT+HhmvtC+LTMTyPGOy8w1mTmcmcMDAwNNy5AktWkU7hHxJlrBfltm3l2an42IwbJ9ENjbrERJ0mQ1uVomgFuAzZn5+bZN9wIry/JK4J7Oy5MkdaKvwbHnAb8PPBYRPyxtfwVcB9wZEVcBTwMfalaiJGmyOg73zPweEBNsXtHpeSVJzfkJVUmqkOEuSRUy3CWpQlWF+9Dqdex+/n97XYYk9VxV4Q7w0GYvq5ek6sJdklRhuE90baYkHU+qC3dJkuEuSVUy3CWpQtWFezjpLkn1hbskyXCXpCoZ7pJUoerCPbzSXZLqC3dJkuEuSVUy3CWpQtWF+8jTP+t1CZLUc9WF+92P7up1CZLUc9WFuyTJcJekKhnuklQhw12SKlRluA+tXsf3tu3rdRmS1DNVhjvA792yodclSFLPVBvuknQ8qz7cH3hsN1/97o5elyFJR1VfrwuYSi/vP8BHb3sUgD8+/4weVyNJR0/Vr9zP/My3Xlu+4aEf97ASSTq6qg73djc8tI0zrlnHqwcOsmXPC70uR5Km1JRMy0TEhcCNwAzg5sy8bioeZ7IOJiy79gEALnjbW/iT889g4Zx+BufO4qSZVc9QSTrOdD3RImIG8EXgPcBO4PsRcW9mbur2YzXx8Ja9PLxl74TbP3Px28mEtw++mR8/+yJb97zIwJx+Bub0s+wts1k4p58doy/x9sE5DM49kVcOHOTAwWR2fx8zTggOHExmnBBkJhGv/3ao9rbxtktSU1PxcvWdwPbM3AEQEd8ALgGOqXA/nL9Zt7nXJQAwOHcWs/v7OHAw2bHvJeb09/Hiy/uZ3d/HKwcO8sr+gwD0953AaQtO6nG1kibr6hXL+J13nNr1805FuC8Gnmlb3wn81tidImIVsArg9NNP7+iB/m31BZx33cMdHXssWji7n32/ePl1bacvOIlTZs8kE3bse4lFc2fx4t5fEAGz+/v42f5XAHh5/0HOXDSnF2VLamDuiW+akvP2bKI5M9cAawCGh4ezk3MsnnciP7nu4q7WJUk1mIqrZXYBp7WtLyltkqSjZCrC/fvAsohYGhEzgcuBe6fgcSRJE+j6tExm7o+IPwO+TetSyFsz84luP44kaWJTMueemfcD90/FuSVJh3fcfEJVko4nhrskVchwl6QKGe6SVKHI7OjzQ90tImIUeLrDwxcCx9sXptrn44N9Pj406fOvZebAeBuOiXBvIiJGMnO413UcTfb5+GCfjw9T1WenZSSpQoa7JFWohnBf0+sCesA+Hx/s8/FhSvo87efcJUm/qoZX7pKkMQx3SarQtA73iLgwIrZGxPaIWN3rejoVEadFxCMRsSkinoiIq0v7goh4MCK2lfv5pT0i4qbS7x9FxDlt51pZ9t8WESt71acjFREzIuIHEXFfWV8aERtK3+4ofzaaiOgv69vL9qG2c1xT2rdGxPt605MjExHzIuKuiNgSEZsj4tzaxzkiPlF+rh+PiNsjYlZt4xwRt0bE3oh4vK2ta+MaEb8ZEY+VY26KI/ni5cycljdaf074SeAMYCbwX8BZva6rw74MAueU5TnAj4GzgL8DVpf21cD1Zfki4AEggOXAhtK+ANhR7ueX5fm97t9h+v5J4J+A+8r6ncDlZfnLwEfL8p8CXy7LlwN3lOWzytj3A0vLz8SMXvfrDfq7FvijsjwTmFfzONP62s2ngBPbxvcPahtn4HzgHODxtraujSvwn2XfKMe+/7A19fpJafBkngt8u239GuCaXtfVpb7dA7wH2AoMlrZBYGtZ/grw4bb9t5btHwa+0tb+uv2OtRutb+laD1wA3Fd+cPcBfWPHmNb3A5xblvvKfjF23Nv3O9ZuwNwSdDGmvdpx5pffqbygjNt9wPtqHGdgaEy4d2Vcy7Ytbe2v22+i23Selhnvi7gX96iWrim/hp4NbAAWZebusmkPsKgsT9T36fac3AB8CjhY1k8BnsvM/WW9vf7X+la2P1/2n059XgqMAl8rU1E3R8TJVDzOmbkL+BzwU2A3rXHbSN3jfEi3xnVxWR7b/oamc7hXJyJmA98EPp6ZL7Rvy9Z/2dVctxoRHwD2ZubGXtdyFPXR+tX9S5l5NvASrV/XX1PhOM8HLqH1H9upwMnAhT0tqgd6Ma7TOdyr+iLuiHgTrWC/LTPvLs3PRsRg2T4I7C3tE/V9Oj0n5wEfjIifAN+gNTVzIzAvIg59Q1h7/a/1rWyfC/w306vPO4GdmbmhrN9FK+xrHud3A09l5mhmvgrcTWvsax7nQ7o1rrvK8tj2NzSdw72aL+Iu73zfAmzOzM+3bboXOPSO+Upac/GH2q8o77ovB54vv/59G3hvRMwvr5jeW9qOOZl5TWYuycwhWmP3cGZ+BHgEuKzsNrbPh56Ly8r+WdovL1dZLAWW0Xrz6ZiTmXuAZyLizNK0AthExeNMazpmeUScVH7OD/W52nFu05VxLdteiIjl5Tm8ou1cE+v1mxAN38C4iNaVJU8C1/a6ngb9+G1av7L9CPhhuV1Ea65xPbANeAhYUPYP4Iul348Bw23n+kNge7ld2eu+HWH/38Uvr5Y5g9Y/2u3APwP9pX1WWd9etp/Rdvy15bnYyhFcRdDjvv4GMFLG+l9oXRVR9TgDfw1sAR4H/pHWFS9VjTNwO633FF6l9RvaVd0cV2C4PH9PAn/PmDflx7v55wckqULTeVpGkjQBw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRV6P8BfooCzDQMSmwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final policy:\n",
            "Printing Policy\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "final values:\n",
            "Printing Values\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.81| 0.00|\n",
            "---------------------------\n",
            " 0.51| 0.16| 0.62| 0.09|\n"
          ]
        }
      ]
    }
  ]
}